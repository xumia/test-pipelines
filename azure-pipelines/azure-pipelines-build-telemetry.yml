# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger: none
pr: none

schedules:
- cron: "0 */2 * * *"
  displayName: Hourly build
  branches:
    include:
    - disable
  always: true

name: $(TeamProject)_$(Build.DefinitionName)_$(SourceBranchName)_$(Date:yyyyMMdd)$(Rev:.r)

stages:
- stage: Build
  pool:
    vmImage: 'ubuntu-latest'
  jobs:
  - job: Build
    timeoutInMinutes: 120
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'
    - script: |
        pip install azure-storage-queue azure-storage-blob
      displayName: Install build tools
    - task: PythonScript@0
      displayName: Publish SONiC telemetry
      inputs:
        scriptSource: 'inline'
        script: |
          import datetime, base64, json, time, os
          from urllib import request
          from azure.storage.queue import QueueClient
          from azure.storage.blob import BlobServiceClient

          QUEUE_NAME="builds"
          CONTAINER="build"
          AZURE_STORAGE_CONNECTION_STRING='$(AZURE_STORAGE_CONNECTION_STRING)'
          BUILD_MESSAGES = 'buildmessages'
          BUILD_INFOS = 'builds'
          BUILD_LOGS = 'buildlogs'
          MESSAGE_PER_PAGE = 10
          MAX_PAGE_COUNT = 30
          blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

          # Upload a list of lines to blob
          def upload_to_blob(lines, blob_prefix, file_prefix=""):
            now = datetime.datetime.now()
            local_file_name = file_prefix + now.strftime("_%Y%m%d-%H%M%S-%f") + '.json'
            with open(local_file_name, "w") as file:
              count = file.write('\n'.join(lines))
            blob_file_name = blob_prefix + now.strftime("/%Y/%m/%d/") + local_file_name
            blob_client = blob_service_client.get_blob_client(container=CONTAINER, blob=blob_file_name)
            with open(local_file_name, "rb") as data:
              blob_client.upload_blob(data)
            os.remove(local_file_name)

          # Download the web content from the url
          def get_response(url):
              for i in range(0, 3):
                  try:
                    print(url)
                    response = request.urlopen(url)
                    data=response.read()
                    encoding = response.info().get_content_charset('utf-8')
                    return data.decode(encoding)
                  except Exception as e:
                    print(e)
                    time.sleep(10)
              raise Exception("failed to get response from {0}".format(url))

          # Get the build logs
          def get_build_logs(timeline_url, build_info):
              timeline_content =  get_response(timeline_url)
              if not timeline_content:
                  return []
              records = json.loads(timeline_content)['records']
              results = []
              for record in records:
                  record['content'] = ""
                  record['buildId'] = build_info['id']
                  record['definitionId'] = build_info['definitionId']
                  record['definitionName'] = build_info['definitionName']
                  if record['log']:
                      log_url = record['log']['url']
                      log = get_response(log_url)
                      record['content'] = log
                  results.append(json.dumps(record))
              return results

          queue_client = QueueClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING, QUEUE_NAME)
          messages = queue_client.receive_messages(messages_per_page=MESSAGE_PER_PAGE, visibility_timeout=3600)
          page = 0
          for msg_batch in messages.by_page():
              page = page + 1
              if page > MAX_PAGE_COUNT:
                  break
              local_file_name = datetime.datetime.now().strftime("_%Y%m%d-%H%M%S-%f") + '.json'
              build_messages = []
              msgs = []
              build_infos = []
              build_logs = []
              msg_count = 0
              for msg in msg_batch:
                  msg_count = msg_count + 1
                  print("process message {} on page {}, current log count {}".format(msg_count, page, len(build_logs)))
                  msgs.append(msg)
                  msg_content = base64.b64decode(msg.content)
                  build = json.loads(msg_content)
                  content = json.dumps(build, separators=(',', ':'))
                  build_messages.append(content)
                  build_url = build['resource']['url']
                  build_content = get_response(build_url)
                  if not build_content:
                      print("Skipped the message for no build content, the message: {}".format(msg_content))
                      continue
                  build_info = json.loads(build_content)
                  build_info['definitionId'] = build_info['definition']['id']
                  build_info['definitionName'] = build_info['definition']['name']
                  build_infos.append(json.dumps(build_info))
                  timeline_url = build_info['_links']['timeline']['href']
                  logs = get_build_logs(timeline_url, build_info)
                  build_logs += logs
              upload_to_blob(build_messages, BUILD_MESSAGES)
              upload_to_blob(build_infos, BUILD_INFOS)
              upload_to_blob(build_logs, BUILD_LOGS)
              for msg in msgs:
                  queue_client.delete_message(msg)
      env:
        AZURE_STORAGE_CONNECTION_STRING: '$(AZURE_STORAGE_CONNECTION_STRING)'
